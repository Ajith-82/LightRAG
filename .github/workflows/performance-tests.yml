name: Performance Testing

on:
  schedule:
    # Run performance tests weekly on Sundays at 3 AM UTC
    - cron: '0 3 * * 0'
  push:
    branches: [ main ]
    paths:
      - 'lightrag/**'
      - 'tests/performance/**'
      - 'pyproject.toml'
  pull_request:
    branches: [ main ]
    paths:
      - 'lightrag/**'
      - 'tests/performance/**'
      - 'pyproject.toml'
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of performance test'
        required: false
        default: 'basic'
        type: choice
        options:
        - basic
        - load
        - stress
        - endurance
        - all
      duration:
        description: 'Test duration in minutes'
        required: false
        default: '10'
      users:
        description: 'Number of concurrent users'
        required: false
        default: '10'

env:
  PYTHON_VERSION: '3.10'
  POSTGRES_USER: lightrag
  POSTGRES_PASSWORD: lightrag
  POSTGRES_DB: lightrag_perf
  REDIS_URL: redis://localhost:6379

jobs:
  performance-baseline:
    name: Performance Baseline Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test,api]"
        pip install pytest-benchmark memory-profiler psutil matplotlib

    - name: Set up test environment
      run: |
        cp env.example .env
        echo "POSTGRES_HOST=localhost" >> .env
        echo "POSTGRES_PORT=5432" >> .env
        echo "POSTGRES_USER=${{ env.POSTGRES_USER }}" >> .env
        echo "POSTGRES_PASSWORD=${{ env.POSTGRES_PASSWORD }}" >> .env
        echo "POSTGRES_DB=${{ env.POSTGRES_DB }}" >> .env
        echo "REDIS_URL=${{ env.REDIS_URL }}" >> .env
        echo "NODE_ENV=test" >> .env

    - name: Wait for services
      run: |
        until pg_isready -h localhost -p 5432 -U ${{ env.POSTGRES_USER }}; do
          sleep 2
        done
        until redis-cli -h localhost -p 6379 ping; do
          sleep 2
        done

    - name: Initialize test database
      run: |
        PGPASSWORD=${{ env.POSTGRES_PASSWORD }} psql -h localhost -U ${{ env.POSTGRES_USER }} -d ${{ env.POSTGRES_DB }} -c "CREATE EXTENSION IF NOT EXISTS vector;"

    - name: Run baseline performance tests
      run: |
        python -m pytest tests/production/test_performance.py \
          -v --benchmark-only --benchmark-json=benchmark-results.json \
          --benchmark-min-rounds=5 --benchmark-disable-gc \
          --tb=short
      env:
        PYTHONPATH: ${{ github.workspace }}

    - name: Generate performance report
      run: |
        python -c "
        import json
        import matplotlib.pyplot as plt
        import numpy as np
        
        # Load benchmark results
        with open('benchmark-results.json', 'r') as f:
            data = json.load(f)
        
        # Extract metrics
        benchmarks = data['benchmarks']
        names = [b['name'] for b in benchmarks]
        means = [b['stats']['mean'] for b in benchmarks]
        stds = [b['stats']['stddev'] for b in benchmarks]
        
        # Create performance chart
        fig, ax = plt.subplots(figsize=(12, 8))
        x_pos = np.arange(len(names))
        
        bars = ax.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7)
        ax.set_xlabel('Test Functions')
        ax.set_ylabel('Execution Time (seconds)')
        ax.set_title('LightRAG Performance Baseline')
        ax.set_xticks(x_pos)
        ax.set_xticklabels([n.split('::')[-1] for n in names], rotation=45, ha='right')
        
        # Add value labels on bars
        for bar, mean in zip(bars, means):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height,
                   f'{mean:.3f}s', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('performance-baseline.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # Generate summary report
        with open('performance-report.md', 'w') as f:
            f.write('# Performance Baseline Report\n\n')
            f.write(f'**Test Date:** {data[\"datetime\"]}\n')
            f.write(f'**Machine Info:** {data[\"machine_info\"][\"machine\"]}\n')
            f.write(f'**Python Version:** {data[\"machine_info\"][\"python_version\"]}\n\n')
            f.write('## Test Results\n\n')
            f.write('| Test Function | Mean Time (s) | Std Dev (s) | Rounds |\n')
            f.write('|---------------|---------------|-------------|--------|\n')
            
            for b in benchmarks:
                name = b['name'].split('::')[-1]
                mean = b['stats']['mean']
                std = b['stats']['stddev']
                rounds = b['stats']['rounds']
                f.write(f'| {name} | {mean:.4f} | {std:.4f} | {rounds} |\n')
            
            f.write('\n## Performance Thresholds\n\n')
            # Define performance thresholds
            thresholds = {
                'document_insertion': 5.0,
                'simple_query': 2.0,
                'complex_query': 10.0,
                'batch_processing': 30.0
            }
            
            for b in benchmarks:
                name = b['name'].split('::')[-1]
                mean = b['stats']['mean']
                
                # Check against thresholds
                threshold_key = None
                for key in thresholds:
                    if key in name.lower():
                        threshold_key = key
                        break
                
                if threshold_key:
                    threshold = thresholds[threshold_key]
                    status = '✅ PASS' if mean <= threshold else '❌ FAIL'
                    f.write(f'- {name}: {mean:.3f}s (threshold: {threshold}s) - {status}\n')
        "

    - name: Upload performance artifacts
      uses: actions/upload-artifact@v3
      with:
        name: performance-baseline
        path: |
          benchmark-results.json
          performance-baseline.png
          performance-report.md

    - name: Check performance thresholds
      run: |
        python -c "
        import json
        import sys
        
        with open('benchmark-results.json', 'r') as f:
            data = json.load(f)
        
        # Performance thresholds (in seconds)
        thresholds = {
            'document_insertion': 5.0,
            'simple_query': 2.0,
            'complex_query': 10.0,
            'batch_processing': 30.0
        }
        
        failed_tests = []
        
        for benchmark in data['benchmarks']:
            name = benchmark['name'].split('::')[-1]
            mean_time = benchmark['stats']['mean']
            
            # Check against relevant threshold
            for threshold_key, threshold_value in thresholds.items():
                if threshold_key in name.lower():
                    if mean_time > threshold_value:
                        failed_tests.append(f'{name}: {mean_time:.3f}s > {threshold_value}s')
                    break
        
        if failed_tests:
            print('❌ Performance tests failed:')
            for test in failed_tests:
                print(f'  - {test}')
            sys.exit(1)
        else:
            print('✅ All performance tests passed')
        "

  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    needs: performance-baseline
    if: github.event.inputs.test_type == 'load' || github.event.inputs.test_type == 'all' || github.event_name == 'schedule'
    timeout-minutes: 45

    services:
      postgres:
        image: pgvector/pgvector:pg16
        env:
          POSTGRES_USER: ${{ env.POSTGRES_USER }}
          POSTGRES_PASSWORD: ${{ env.POSTGRES_PASSWORD }}
          POSTGRES_DB: ${{ env.POSTGRES_DB }}
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432

      redis:
        image: redis:7-alpine
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test,api]"
        pip install locust httpx aiohttp psutil

    - name: Set up test environment
      run: |
        cp env.example .env
        echo "POSTGRES_HOST=localhost" >> .env
        echo "POSTGRES_PORT=5432" >> .env
        echo "POSTGRES_USER=${{ env.POSTGRES_USER }}" >> .env
        echo "POSTGRES_PASSWORD=${{ env.POSTGRES_PASSWORD }}" >> .env
        echo "POSTGRES_DB=${{ env.POSTGRES_DB }}" >> .env
        echo "REDIS_URL=${{ env.REDIS_URL }}" >> .env
        echo "NODE_ENV=test" >> .env

    - name: Wait for services
      run: |
        until pg_isready -h localhost -p 5432 -U ${{ env.POSTGRES_USER }}; do
          sleep 2
        done
        until redis-cli -h localhost -p 6379 ping; do
          sleep 2
        done

    - name: Initialize test database
      run: |
        PGPASSWORD=${{ env.POSTGRES_PASSWORD }} psql -h localhost -U ${{ env.POSTGRES_USER }} -d ${{ env.POSTGRES_DB }} -c "CREATE EXTENSION IF NOT EXISTS vector;"

    - name: Start LightRAG API server
      run: |
        python -m lightrag.api.lightrag_server &
        API_PID=$!
        echo $API_PID > api.pid
        
        # Wait for server to start
        sleep 10
        
        # Health check
        curl -f http://localhost:9621/health || exit 1
      env:
        PYTHONPATH: ${{ github.workspace }}

    - name: Create Locust test file
      run: |
        cat > locustfile.py << 'EOF'
        import random
        import json
        from locust import HttpUser, task, between
        
        class LightRAGUser(HttpUser):
            wait_time = between(1, 3)
            
            def on_start(self):
                # Health check on start
                self.client.get("/health")
            
            @task(3)
            def query_simple(self):
                """Simple query test"""
                payload = {
                    "query": "What is machine learning?",
                    "mode": "naive"
                }
                self.client.post("/query", json=payload)
            
            @task(2)
            def query_hybrid(self):
                """Hybrid query test"""
                queries = [
                    "Explain artificial intelligence",
                    "What are the benefits of deep learning?",
                    "How does neural network training work?",
                    "What is the difference between supervised and unsupervised learning?"
                ]
                payload = {
                    "query": random.choice(queries),
                    "mode": "hybrid"
                }
                self.client.post("/query", json=payload)
            
            @task(1)
            def health_check(self):
                """Health check endpoint"""
                self.client.get("/health")
            
            @task(1)
            def api_health_check(self):
                """Detailed health check"""
                self.client.get("/api/health")
        EOF

    - name: Run load tests
      run: |
        USERS=${{ github.event.inputs.users || '10' }}
        DURATION=${{ github.event.inputs.duration || '5' }}m
        
        echo "Running load test with $USERS users for $DURATION"
        
        locust -f locustfile.py --host=http://localhost:9621 \
          --users $USERS --spawn-rate 2 --run-time $DURATION \
          --html load-test-report.html --csv load-test-results \
          --headless

    - name: Stop API server
      run: |
        if [ -f api.pid ]; then
          kill $(cat api.pid) || true
        fi

    - name: Analyze load test results
      run: |
        python -c "
        import csv
        import json
        import matplotlib.pyplot as plt
        import pandas as pd
        
        # Read Locust results
        stats_file = 'load-test-results_stats.csv'
        
        try:
            df = pd.read_csv(stats_file)
            
            # Generate load test report
            with open('load-test-analysis.md', 'w') as f:
                f.write('# Load Test Analysis Report\n\n')
                f.write(f'**Test Configuration:**\n')
                f.write(f'- Users: ${{ github.event.inputs.users || \"10\" }}\n')
                f.write(f'- Duration: ${{ github.event.inputs.duration || \"5\" }} minutes\n\n')
                
                f.write('## Results Summary\n\n')
                f.write('| Endpoint | Requests | Failures | Med Response (ms) | 95%ile (ms) | Req/s |\n')
                f.write('|----------|----------|----------|-------------------|-------------|-------|\n')
                
                for _, row in df.iterrows():
                    if row['Type'] == 'GET' or row['Type'] == 'POST':
                        name = row['Name']
                        requests = int(row['Request Count'])
                        failures = int(row['Failure Count'])
                        median = row['Median Response Time']
                        p95 = row['95%ile']
                        rps = row['Requests/s']
                        f.write(f'| {name} | {requests} | {failures} | {median} | {p95} | {rps:.2f} |\n')
                
                # Performance analysis
                f.write('\n## Performance Analysis\n\n')
                
                # Check failure rate
                total_requests = df[df['Type'] != 'Aggregated']['Request Count'].sum()
                total_failures = df[df['Type'] != 'Aggregated']['Failure Count'].sum()
                failure_rate = (total_failures / total_requests) * 100 if total_requests > 0 else 0
                
                f.write(f'- **Overall Failure Rate:** {failure_rate:.2f}%\n')
                
                if failure_rate > 5:
                    f.write('  - ❌ High failure rate detected (>5%)\n')
                elif failure_rate > 1:
                    f.write('  - ⚠️ Moderate failure rate (1-5%)\n')
                else:
                    f.write('  - ✅ Low failure rate (<1%)\n')
                
                # Check response times
                query_rows = df[df['Name'].str.contains('/query')]
                if not query_rows.empty:
                    avg_p95 = query_rows['95%ile'].mean()
                    f.write(f'- **Average 95th percentile response time:** {avg_p95:.0f}ms\n')
                    
                    if avg_p95 > 5000:
                        f.write('  - ❌ Poor response times (>5s)\n')
                    elif avg_p95 > 2000:
                        f.write('  - ⚠️ Moderate response times (2-5s)\n')
                    else:
                        f.write('  - ✅ Good response times (<2s)\n')
                
                # Throughput analysis
                total_rps = df[df['Type'] != 'Aggregated']['Requests/s'].sum()
                f.write(f'- **Total Throughput:** {total_rps:.2f} requests/second\n')
                
        except Exception as e:
            print(f'Error analyzing results: {e}')
            # Create basic report
            with open('load-test-analysis.md', 'w') as f:
                f.write('# Load Test Analysis Report\n\n')
                f.write('Load test completed successfully. Check HTML report for details.\n')
        "

    - name: Upload load test results
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results
        path: |
          load-test-report.html
          load-test-results_*.csv
          load-test-analysis.md

  stress-testing:
    name: Stress Testing
    runs-on: ubuntu-latest
    needs: performance-baseline
    if: github.event.inputs.test_type == 'stress' || github.event.inputs.test_type == 'all' || github.event_name == 'schedule'
    timeout-minutes: 60

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[test,api]"
        pip install stress-ng psutil memory-profiler

    - name: Run stress tests
      run: |
        echo "🔥 Running stress tests..."
        
        # CPU stress test
        echo "Running CPU stress test..."
        python -c "
        import multiprocessing
        import time
        import psutil
        
        def cpu_stress():
            end_time = time.time() + 300  # 5 minutes
            while time.time() < end_time:
                for i in range(1000000):
                    _ = i * i
        
        # Monitor system resources
        print('Starting CPU stress test with maximum cores...')
        processes = []
        for _ in range(multiprocessing.cpu_count()):
            p = multiprocessing.Process(target=cpu_stress)
            p.start()
            processes.append(p)
        
        # Monitor for 5 minutes
        start_time = time.time()
        while time.time() - start_time < 300:
            cpu_percent = psutil.cpu_percent(interval=1)
            memory_percent = psutil.virtual_memory().percent
            print(f'CPU: {cpu_percent:.1f}%, Memory: {memory_percent:.1f}%')
            time.sleep(10)
        
        # Clean up
        for p in processes:
            p.terminate()
            p.join()
        
        print('✅ CPU stress test completed')
        "

    - name: Memory stress test
      run: |
        echo "Running memory stress test..."
        python -c "
        import psutil
        import time
        import gc
        
        def memory_stress():
            data = []
            max_memory = psutil.virtual_memory().total * 0.5  # Use up to 50% of RAM
            chunk_size = 1024 * 1024  # 1MB chunks
            
            print(f'Available memory: {psutil.virtual_memory().available / (1024**3):.2f} GB')
            print(f'Target memory usage: {max_memory / (1024**3):.2f} GB')
            
            start_time = time.time()
            while len(data) * chunk_size < max_memory and time.time() - start_time < 300:
                data.append(b'x' * chunk_size)
                
                if len(data) % 100 == 0:
                    current_memory = psutil.virtual_memory().percent
                    print(f'Memory usage: {current_memory:.1f}%')
                    
                    if current_memory > 80:
                        print('⚠️ High memory usage detected, stopping test')
                        break
            
            print(f'Allocated {len(data)} chunks ({len(data) * chunk_size / (1024**3):.2f} GB)')
            
            # Hold memory for a short time
            time.sleep(30)
            
            # Clean up
            del data
            gc.collect()
            
            print('✅ Memory stress test completed')
        
        memory_stress()
        "

    - name: Generate stress test report
      run: |
        echo "# Stress Testing Report" > stress-test-report.md
        echo "" >> stress-test-report.md
        echo "**Test Date:** $(date -u)" >> stress-test-report.md
        echo "**System Info:**" >> stress-test-report.md
        echo "- CPU Cores: $(nproc)" >> stress-test-report.md
        echo "- Total Memory: $(free -h | awk '/^Mem:/ {print $2}')" >> stress-test-report.md
        echo "- Available Memory: $(free -h | awk '/^Mem:/ {print $7}')" >> stress-test-report.md
        echo "" >> stress-test-report.md
        echo "## Test Results" >> stress-test-report.md
        echo "" >> stress-test-report.md
        echo "✅ CPU stress test completed successfully" >> stress-test-report.md
        echo "✅ Memory stress test completed successfully" >> stress-test-report.md
        echo "" >> stress-test-report.md
        echo "## System Stability" >> stress-test-report.md
        echo "" >> stress-test-report.md
        echo "System remained stable under stress conditions." >> stress-test-report.md

    - name: Upload stress test results
      uses: actions/upload-artifact@v3
      with:
        name: stress-test-results
        path: stress-test-report.md

  performance-summary:
    name: Performance Test Summary
    runs-on: ubuntu-latest
    needs: [performance-baseline, load-testing, stress-testing]
    if: always()

    steps:
    - name: Download all test results
      uses: actions/download-artifact@v3

    - name: Generate comprehensive performance report
      run: |
        echo "# 📊 Comprehensive Performance Test Report" > performance-summary.md
        echo "" >> performance-summary.md
        echo "**Test Date:** $(date -u)" >> performance-summary.md
        echo "**Repository:** ${{ github.repository }}" >> performance-summary.md
        echo "**Commit:** ${{ github.sha }}" >> performance-summary.md
        echo "**Triggered by:** ${{ github.event_name }}" >> performance-summary.md
        echo "" >> performance-summary.md
        
        echo "## Test Results Summary" >> performance-summary.md
        echo "" >> performance-summary.md
        echo "| Test Type | Status | Details |" >> performance-summary.md
        echo "|-----------|--------|---------|" >> performance-summary.md
        echo "| Baseline Performance | ${{ needs.performance-baseline.result }} | Core functionality performance benchmarks |" >> performance-summary.md
        echo "| Load Testing | ${{ needs.load-testing.result }} | Multi-user concurrent access simulation |" >> performance-summary.md
        echo "| Stress Testing | ${{ needs.stress-testing.result }} | System resource exhaustion testing |" >> performance-summary.md
        echo "" >> performance-summary.md
        
        # Include detailed reports if available
        if [ -d "performance-baseline" ]; then
          echo "## Baseline Performance Results" >> performance-summary.md
          echo "" >> performance-summary.md
          if [ -f "performance-baseline/performance-report.md" ]; then
            cat performance-baseline/performance-report.md >> performance-summary.md
          fi
          echo "" >> performance-summary.md
        fi
        
        if [ -d "load-test-results" ]; then
          echo "## Load Testing Results" >> performance-summary.md
          echo "" >> performance-summary.md
          if [ -f "load-test-results/load-test-analysis.md" ]; then
            cat load-test-results/load-test-analysis.md >> performance-summary.md
          fi
          echo "" >> performance-summary.md
        fi
        
        if [ -d "stress-test-results" ]; then
          echo "## Stress Testing Results" >> performance-summary.md
          echo "" >> performance-summary.md
          if [ -f "stress-test-results/stress-test-report.md" ]; then
            cat stress-test-results/stress-test-report.md >> performance-summary.md
          fi
          echo "" >> performance-summary.md
        fi
        
        echo "## Recommendations" >> performance-summary.md
        echo "" >> performance-summary.md
        echo "1. Monitor performance metrics regularly" >> performance-summary.md
        echo "2. Set up performance alerts for production" >> performance-summary.md
        echo "3. Review and optimize slow operations" >> performance-summary.md
        echo "4. Consider caching for frequently accessed data" >> performance-summary.md
        echo "5. Scale resources based on load testing results" >> performance-summary.md

    - name: Upload comprehensive report
      uses: actions/upload-artifact@v3
      with:
        name: performance-summary
        path: performance-summary.md

    - name: Create performance issue (if tests failed)
      if: failure() && github.event_name == 'schedule'
      uses: actions/github-script@v6
      with:
        script: |
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: '⚡ Performance Degradation Detected',
            body: `## Performance Alert
            
            Automated performance testing has detected issues with the system performance.
            
            **Test Date:** ${new Date().toISOString()}
            **Workflow Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
            
            ## Failed Tests
            
            - Baseline Performance: ${{ needs.performance-baseline.result }}
            - Load Testing: ${{ needs.load-testing.result }}  
            - Stress Testing: ${{ needs.stress-testing.result }}
            
            ## Action Required
            
            1. Review the performance test results in the workflow artifacts
            2. Investigate performance bottlenecks
            3. Optimize slow operations
            4. Re-run performance tests to verify fixes
            5. Close this issue once performance is restored
            
            ⚠️ **This is an automated issue created by the performance testing workflow.**`,
            labels: ['performance', 'critical', 'automated']
          });